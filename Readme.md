# FastAPI RAG with ChromaDB and Ollama

This project is a FastAPI application for Retrieval-Augmented Generation (RAG) that runs in Docker containers. It provides endpoints to store documents and query them using embeddings and a large language model (LLM). The application uses ChromaDB for vector storage and Ollama for both embeddings (nomic-embed-text) and LLM (llama3).

## Features

- **/document** (`POST`): Store a document and its ID in ChromaDB.
- **/query** (`POST`): Query for relevant documents and get an answer generated by the LLM, along with a list of relevant document IDs.
- Uses Ollama for embeddings and LLM, each running in a separate container.
- ChromaDB runs in its own container for vector storage.

## Project Structure

```
docker-compose.yml
prompt
app/
    Dockerfile
    main.py
    requirements.txt
example documents/
    cat.txt
```

## Getting Started

### Prerequisites

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

### Build and Run

1. **Clone the repository:**

   ```sh
   git clone <your-repo-url>
   cd FastAPI-RAG
   ```

2. **Start the services:**

   ```sh
   docker-compose up --build
   ```

   This will start three containers:
   - `app`: The FastAPI application
   - `chromadb`: The ChromaDB vector database
   - `ollama`: The Ollama server for embeddings and LLM

3. **Access the API:**

   The FastAPI app will be available at [http://localhost:8000](http://localhost:8000).

## API Endpoints

### Store a Document

- **POST** `/document`
- **Body:**
  ```json
  {
    "id": "doc1",
    "content": "This is the document content."
  }
  ```
- **Response:**
  ```json
  {
    "message": "Document with id 'doc1' processed and stored."
  }
  ```

### Query Documents

- **POST** `/query`
- **Body:**
  ```json
  {
    "query": "What is the story about?"
  }
  ```
- **Response:**
  ```json
  {
    "answer": "The answer generated by the LLM.",
    "relevant_ids": ["doc1"]
  }
  ```

## Example

To add the example document:

```sh
curl -X POST http://localhost:8000/document \
  -H "Content-Type: application/json" \
  -d '{"id": "cat", "content": "This is a story about a cat named Murphy. He was a Bengal cat, and very intelligent."}'
```

To query:

```sh
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Who is Murphy?"}'
```

## Notes

- The application automatically pulls the required Ollama models (`nomic-embed-text` and `llama3`) on startup if they are not present.
- All configuration (hostnames, ports) is managed via `docker-compose.yml` and environment variables.

## License

MIT License